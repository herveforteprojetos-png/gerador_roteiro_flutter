import 'dart:math';
import 'package:dio/dio.dart';
import 'package:flutter/foundation.dart';

/// ü§ñ LlmClient - Cliente para comunica√ß√£o com APIs de LLM (Gemini)
///
/// Respons√°vel por:
/// - Inicializa√ß√£o e configura√ß√£o do cliente HTTP
/// - Gest√£o de API Keys
/// - Chamadas √† API Gemini
/// - M√©todos simplificados: `generateText` e `generateJson`
///
/// Parte da refatora√ß√£o SOLID do GeminiService v7.6.64
class LlmClient {
  final Dio _dio;
  final String _instanceId;

  /// üìù Helper padronizado para logs (mant√©m emojis em debug, limpa em produ√ß√£o)
  static void _log(String message, {String level = 'info'}) {
    if (kDebugMode) {
      debugPrint(message);
    } else if (level == 'error' || level == 'critical') {
      final cleaned = message
          .replaceAll(RegExp(r'[üö®üî•‚úÖ‚ùå‚ö†Ô∏èüí°üìäüéØüìùüîóüìöü§ñüåê]'), '')
          .trim();
      debugPrint('[${level.toUpperCase()}] $cleaned');
    }
  }

  /// Modelos dispon√≠veis no Gemini
  static const String modelFlash = 'gemini-2.5-flash';
  static const String modelPro = 'gemini-2.5-pro';
  static const String modelUltra = 'gemini-3-pro-preview';

  /// Construtor com inje√ß√£o de depend√™ncias opcional
  LlmClient({Dio? dio, String? instanceId})
    : _dio = dio ?? _createDefaultDio(),
      _instanceId = instanceId ?? _genInstanceId();

  /// Cria inst√¢ncia padr√£o do Dio com configura√ß√µes otimizadas
  static Dio _createDefaultDio() {
    return Dio(
      BaseOptions(
        connectTimeout: const Duration(seconds: 45),
        receiveTimeout: const Duration(minutes: 5),
        sendTimeout: const Duration(seconds: 45),
      ),
    );
  }

  /// Gera ID √∫nico para a inst√¢ncia
  static String _genInstanceId() {
    final random = Random();
    return 'llm_${random.nextInt(9999).toString().padLeft(4, '0')}';
  }

  /// üéØ Helper para selecionar modelo baseado no qualityMode
  ///
  /// - 'flash': R√°pido e eficiente (gemini-2.5-flash)
  /// - 'pro': M√°xima qualidade (gemini-2.5-pro) - PADR√ÉO
  /// - 'ultra': Modelo mais avan√ßado (gemini-3-pro-preview)
  static String getModelForQuality(String qualityMode) {
    switch (qualityMode.toLowerCase()) {
      case 'flash':
        return modelFlash;
      case 'ultra':
        return modelUltra;
      case 'pro':
      default:
        return modelPro;
    }
  }

  /// üéØ v7.6.169: Sele√ß√£o H√çBRIDA de modelo (Flash com fallback para Pro em blocos finais)
  ///
  /// ESTRAT√âGIA H√çBRIDA (somente para Flash):
  /// - Flash (qualityMode='flash'):
  ///   * Blocos 1-6: gemini-2.5-flash (contexto pequeno, funciona bem)
  ///   * Blocos 7-12: gemini-2.0-pro (contexto grande, Flash ignora limites)
  /// - Pro/Ultra: mant√©m o mesmo modelo para todos os blocos
  ///
  /// [qualityMode]: Modo de qualidade selecionado pelo usu√°rio
  /// [blockNumber]: N√∫mero do bloco atual (1-12)
  /// [totalBlocks]: Total de blocos da hist√≥ria
  ///
  /// Retorna: Nome do modelo a ser usado neste bloco espec√≠fico
  static String getModelForBlock({
    required String qualityMode,
    required int blockNumber,
    required int totalBlocks,
  }) {
    final mode = qualityMode.toLowerCase();
    
    // Pro e Ultra: sem mudan√ßas, mesmo modelo do in√≠cio ao fim
    if (mode == 'pro' || mode == 'ultra') {
      return getModelForQuality(qualityMode);
    }
    
    // Flash: H√çBRIDO - Pro para blocos finais (contexto > 116k chars)
    if (mode == 'flash') {
      // Threshold: usar Pro a partir de ~60% dos blocos (quando contexto fica grande)
      final switchThreshold = (totalBlocks * 0.6).ceil();
      
      if (blockNumber >= switchThreshold) {
        if (kDebugMode) {
          debugPrint(
            'üîÑ v7.6.169 H√çBRIDO: Bloco $blockNumber/$totalBlocks usando Pro (contexto grande)',
          );
        }
        return modelPro; // Usar Pro para blocos finais
      } else {
        return modelFlash; // Usar Flash para blocos iniciais
      }
    }
    
    // Fallback: usar modelo padr√£o
    return getModelForQuality(qualityMode);
  }

  /// üîß Gera texto usando a API Gemini
  ///
  /// [prompt]: O prompt a ser enviado
  /// [apiKey]: Chave da API Gemini
  /// [model]: Modelo a ser usado (use [getModelForQuality] para obter)
  /// [maxTokens]: M√°ximo de tokens na resposta
  /// [temperature]: Temperatura (criatividade) - padr√£o ajustado por modelo
  ///
  /// Retorna: Texto gerado ou string vazia em caso de erro
  Future<String> generateText({
    required String prompt,
    required String apiKey,
    required String model,
    int maxTokens = 8192,
    double? temperature,
  }) async {
    try {
      // Ajustar temperatura baseado no modelo se n√£o especificado
      final effectiveTemperature = temperature ?? _getDefaultTemperature(model);

      final response = await _makeRequest(
        apiKey: apiKey,
        model: model,
        prompt: prompt,
        maxTokens: maxTokens,
        temperature: effectiveTemperature,
      );

      return response ?? '';
    } catch (e) {
      _log('‚ùå Erro em generateText: $e', level: 'error');
      rethrow;
    }
  }

  /// üéØ Obt√©m temperatura padr√£o otimizada por modelo
  double _getDefaultTemperature(String model) {
    if (model == modelFlash) {
      // Flash: temperatura balanceada (0.6 causava muitas repeti√ß√µes)
      return 0.7;
    } else if (model == modelPro) {
      // Pro: temperatura alta para m√°xima criatividade
      return 0.8;
    } else {
      // Ultra: temperatura balanceada
      return 0.7;
    }
  }

  /// üîß Gera JSON estruturado usando a API Gemini
  ///
  /// √ötil para extra√ß√£o de dados estruturados (ex: World State)
  ///
  /// [prompt]: O prompt a ser enviado (deve instruir formato JSON)
  /// [apiKey]: Chave da API Gemini
  /// [model]: Modelo a ser usado
  /// [maxTokens]: M√°ximo de tokens na resposta
  ///
  /// Retorna: Texto JSON ou string vazia em caso de erro
  Future<String> generateJson({
    required String prompt,
    required String apiKey,
    required String model,
    int maxTokens = 2048,
  }) async {
    try {
      // Temperatura mais baixa para JSON consistente
      final response = await _makeRequest(
        apiKey: apiKey,
        model: model,
        prompt: prompt,
        maxTokens: maxTokens,
        temperature: 0.3, // Baixa temperatura para JSON estruturado
      );

      return response ?? '';
    } catch (e) {
      _log('‚ùå Erro em generateJson: $e', level: 'error');
      rethrow;
    }
  }

  /// üîß Faz requisi√ß√£o √† API Gemini
  ///
  /// M√©todo interno que realiza a chamada HTTP
  Future<String?> _makeRequest({
    required String apiKey,
    required String model,
    required String prompt,
    required int maxTokens,
    double temperature = 0.8,
  }) async {
    try {
      // Ajustar maxTokens para limites da API
      final adjustedMaxTokens = maxTokens < 8192
          ? 8192
          : min(maxTokens * 2, 32768);

      final resp = await _dio.post(
        'https://generativelanguage.googleapis.com/v1beta/models/$model:generateContent',
        queryParameters: {'key': apiKey},
        data: {
          'contents': [
            {
              'parts': [
                {'text': prompt},
              ],
            },
          ],
          'generationConfig': {
            'temperature': temperature,
            'topK': 40,
            'topP': 0.95,
            'maxOutputTokens': adjustedMaxTokens,
          },
        },
      );

      if (kDebugMode) {
        debugPrint('[$_instanceId] Status Code: ${resp.statusCode}');
      }

      // Verificar erro na resposta
      if (resp.data['error'] != null) {
        debugPrint('[$_instanceId] API Error: ${resp.data['error']}');
        throw Exception('API Error: ${resp.data['error']['message']}');
      }

      // Verificar bloqueio de conte√∫do
      final promptFeedback = resp.data['promptFeedback'];
      if (promptFeedback != null && promptFeedback['blockReason'] != null) {
        final blockReason = promptFeedback['blockReason'];
        _log('üö´ CONTE√öDO BLOQUEADO - Raz√£o: $blockReason', level: 'error');
        return null;
      }

      // Verificar finish reason
      final finishReason = resp.data['candidates']?[0]?['finishReason'];
      if (finishReason == 'MAX_TOKENS' && kDebugMode) {
        debugPrint(
          '[$_instanceId] Aviso - Resposta cortada por limite de tokens',
        );
      }

      // Extrair texto da resposta
      String? result;
      final candidate = resp.data['candidates']?[0];

      if (candidate != null) {
        result = candidate['content']?['parts']?[0]?['text'] as String?;

        if (result == null || result.isEmpty) {
          result = candidate['content']?['text'] as String?;
        }

        if (result == null || result.isEmpty) {
          result = candidate['text'] as String?;
        }
      }

      if (kDebugMode) {
        debugPrint(
          '[$_instanceId] Extracted text: ${result?.length ?? 0} chars',
        );
      }

      // Limpar texto de marca√ß√µes indesejadas
      if (result != null) {
        result = _cleanGeneratedText(result);
      }

      return result;
    } catch (e) {
      _log('‚ùå Erro na requisi√ß√£o API: $e', level: 'error');
      
      // üö® Tratamento especial para erro 429 (Rate Limit)
      if (e.toString().contains('429')) {
        _log('‚ö†Ô∏è Rate Limit atingido - aguarde antes de nova tentativa', level: 'warning');
      }
      
      rethrow;
    }
  }

  /// Limpa texto de marca√ß√µes indesejadas
  String _cleanGeneratedText(String text) {
    return text
        .replaceAll(RegExp(r'CONTINUA√á√ÉO:\s*', caseSensitive: false), '')
        .replaceAll(RegExp(r'CONTEXTO FINAL:\s*', caseSensitive: false), '')
        .replaceAll(RegExp(r'\n\n\n+'), '\n\n')
        .trim();
  }

  /// üîß Verifica se a API key √© v√°lida fazendo uma requisi√ß√£o simples
  Future<bool> validateApiKey(String apiKey) async {
    try {
      final response = await _dio.post(
        'https://generativelanguage.googleapis.com/v1beta/models/$modelFlash:generateContent',
        queryParameters: {'key': apiKey},
        data: {
          'contents': [
            {
              'parts': [
                {'text': 'Hello'},
              ],
            },
          ],
          'generationConfig': {'maxOutputTokens': 10},
        },
      );

      return response.statusCode == 200;
    } catch (e) {
      return false;
    }
  }

  /// Libera recursos
  void dispose() {
    _dio.close();
  }
}
